{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 The Support Vector Machine (SVM) (part 2/2)\n",
    "-------------------\n",
    "\n",
    "Again, we assume that you have heard of and also known the basics of the **SVM**. If not, please feel free to walk through the basics of the **SVM** in another notebook **The Support Vector Machine (SVM) (part 1/2)**, where you start with artificially generated dummy data and try to understand how the **SVM** works:\n",
    "\n",
    "- 2.0 Understanding the SVM through dummy dataset\n",
    "- 2.1 Applying the SVM through existing module(s)\n",
    "- 2.2 Dealing with non-linearity\n",
    "\n",
    "In this notebook, you will apply the **SVM** to the real world RS and GIS dataset. Structure of this session will be:\n",
    "\n",
    "- 2.3 The SVM on real world GIS and Remote Sensing datasets\n",
    "- 2.4 Model evaluation\n",
    "\n",
    "<font color='blue'> ***Throughout the notebook, you will encounter Questions, highlighted in blue. Please feel free to discuss them with your classmates and advisors***</font>\n",
    "\n",
    "<font color='green'> ***Throughout the notebook, you will also encounter shorter and longer Exercises, highlighted in green.***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The SVM on real world GIS and Remote Sensing datasets.\n",
    "\n",
    "You will be working with the same dataset containing sample land use and land cover (LULC) information. You will train and test your **SVM** on these sample LULC classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before reading the data we need to first clone the data on Github to our Colab workspace\n",
    "!git clone https://github.com/jonwangio/uu_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same real world dataset as you have already encountered in the previous sessions.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "% pip install geopandas\n",
    "import geopandas as gpd\n",
    "% pip install rasterio\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# The data contains sample LULC areas around dutch provinces North Holland and Utrecht.\n",
    "aoi = gpd.read_file('uu_ml/data/aoi_NL_5_classes.shp')\n",
    "\n",
    "print (aoi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a visualization of the sample LULC areas superimposed on a satellite image of part of the Netherlands\n",
    "file_location = 'uu_ml/data/b5_2015.TIF'\n",
    "b5_2020 = rasterio.open(file_location, nodata=0)\n",
    "\n",
    "# We also prepare the color codes for visualization\n",
    "colors = [(257, 71, 27), (98, 93, 78), (14, 79, 58), (26, 0, 50), (75, 90, 85), (347, 72, 60), (246, 79, 60)]\n",
    "cols = []\n",
    "for col in colors:\n",
    "    pal = sns.light_palette(col, input=\"husl\", n_colors=4)\n",
    "    for rgb in pal[1:]:\n",
    "        cols.append(rgb)\n",
    "\n",
    "# A preview of color codes. Please delete the triple quotation marks to run the code.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "for i, c in enumerate(cols):\n",
    "    ax.add_artist(plt.Circle((i, 0), 0.4, color=c))\n",
    "    plt.text(i, -1, i, horizontalalignment='center')\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect(1)\n",
    "    ax.autoscale()\n",
    "    plt.xlim(-1.25,43.25)\n",
    "    plt.ylim(-1,1)\n",
    "\n",
    "\n",
    "# Assign color codes to LULC types \n",
    "symbology = {'Agriculture': cols[5],\n",
    "             'Clear water': cols[20],\n",
    "             'Deciduous forest': cols[13],\n",
    "             'Residential': cols[17],\n",
    "             'Sand': cols[11]}\n",
    "\n",
    "# Visualize\n",
    "fig,ax = plt.subplots(1,1, figsize=(10,10))\n",
    "show(b5_2020, ax=ax, cmap='gray', alpha=0.25)\n",
    "aoi.plot(ax=ax, column='land_cover', legend=True, color=aoi['land_cover'].map(symbology))\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=5, color=color) for color in symbology.values()]\n",
    "leg_points = ax.legend(custom_points, symbology.keys(), loc='upper right', frameon=False)\n",
    "ax.add_artist(leg_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training and test datasets from the sample LULC types in AOIs. We will start with fewer bands, let's say two, and then add more bands to evaluate the model performance. Hence, in the sense of **supervised learning**, the inputs will be the image bands, and the manually delineated LULC types in the AOIs will be the labels. In our case, we will be stacking all of them and split the stack into training and test sets, so that the inputs and labels will be split accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and functionalities\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imread\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# As before, we start with loading and stacking the image bands.\n",
    "# Again, we start with 2 bands that appeared to be useful for the small dataset.\n",
    "file_list = ['uu_ml/data/b5_2015.TIF', 'uu_ml/data/b6_2015.TIF']  # List to store file names\n",
    "\n",
    "# You can print to see how the file_list looks like\n",
    "print(file_list)\n",
    "\n",
    "# Read the files and stack them together by calling their names\n",
    "# Use the 'for' loop to iterate over the names to read files\n",
    "stack = np.array([])  # Empty array to store the stacked images\n",
    "for file in file_list:\n",
    "    img = imread(file)  # Read each image file\n",
    "    print(img.shape)  # Each time, also check the size of the image\n",
    "    \n",
    "    # In order to do clustering, image should be reshaped into a single column\n",
    "    img_col = img.reshape(-1, 1)\n",
    "    \n",
    "    # Each time put the reshaped image into the stack\n",
    "    stack = np.hstack((stack,img_col)) if stack.size else img_col\n",
    "    # Also to check the size of the stack\n",
    "    print(stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to rasterize our manually delineated LULC types in the AOIs as we did in previous sessions\n",
    "# Use the rasterio again to rasterize the *.shp file\n",
    "\n",
    "from rasterio import features\n",
    "\n",
    "# Labels from the AOIs\n",
    "aoi = gpd.read_file('uu_ml/data/aoi_NL_5_classes.shp')\n",
    "aoi['aoi_cat'] = pd.Categorical(aoi['class'])\n",
    "\n",
    "# Rasterize\n",
    "rst = rasterio.open('uu_ml/data/b5_2015.TIF')  # Base image to rasterize the *.shp\n",
    "meta = rst.meta.copy()  # Copy metadata from the base image\n",
    "meta.update(compress='lzw')\n",
    "\n",
    "# Burn the AOIs *.shp file into raster and save it\n",
    "out_rst = 'uu_ml/data/aoi_rasterized.tif'\n",
    "with rasterio.open(out_rst, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # Create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(aoi.geometry, aoi.aoi_cat))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "    out.write_band(1, burned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rasterized LULC types in the AOI and concatenate it together with the images\n",
    "\n",
    "aoi_rst = rasterio.open('uu_ml/data/aoi_rasterized.tif').read(1)\n",
    "\n",
    "# Stack the label with the input bands\n",
    "data = np.c_[stack, aoi_rst.reshape(-1,)]\n",
    "\n",
    "# Of course, we are only interested in pixels with LULC type labelled\n",
    "data = data[np.where(data[:,data.shape[1]-1]!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall our function for preparing training and test datasets.\n",
    "# This time we re-write it a little bit to let the users of the function to split the data into training and test sets.\n",
    "\n",
    "def trainTestSplit(x, y, training_proportion):\n",
    "    data = np.c_[x, y]\n",
    "    np.random.shuffle(data)  # Shuffle the data so that LULC types can spread over training and test sets\n",
    "    x_train = data[:int(training_proportion*len(data)), :2]  # 70% of data for training\n",
    "    x_test = data[int(training_proportion*len(data)):, :2]  # 30% for testing\n",
    "    y_train = data[:int(training_proportion*len(data)), 2:].reshape(-1,)  # 70% of data for training\n",
    "    y_test = data[int(training_proportion*len(data)):, 2:].reshape(-1,)  # 30% for testing\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# This time, use a very small proportion of the data for training, say, 30%.\n",
    "X_train, Y_train, X_test, Y_test = trainTestSplit(data[:,:-1], data[:,-1], 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign color codes to LULC types for visualizing the datasets\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "symbology2 = {11: cols[20],\n",
    "              21: cols[11],\n",
    "              31: cols[17],\n",
    "              52: cols[5],\n",
    "              61: cols[13]}\n",
    "\n",
    "cm = ListedColormap(symbology2.values())\n",
    "imin = min(symbology2)  # Colormap range\n",
    "imax = max(symbology2)\n",
    "\n",
    "# Visualize\n",
    "classes = ['Clear water', 'Sand', 'Residential', 'Deciduous forest', 'Agriculture']\n",
    "\n",
    "fig1,(ax1, ax2) = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "scatter1 = ax1.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=cm, vmin=imin, vmax=imax, label='LULC types')\n",
    "ax1.set_title('Training data')\n",
    "ax1.legend(handles=scatter1.legend_elements()[0], labels=classes)\n",
    "\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm, vmin=imin, vmax=imax, label='LULC types')\n",
    "ax2.set_title('Test data')\n",
    "ax2.legend(handles=scatter1.legend_elements()[0], labels=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>***Question 0.1: How well the training and test datasets resembling the patterns of each other? Recall that we have shuffle the dataset before splitting it into training and test sets. How would this shuffling impact the split? Please try to modify the cell above regarding the data splitting, and try to see the impact. And do you think this would impact further your training with the SVM?***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prepared and also visualized the training and test datasets, we can assess the performance the **SVM**. Let's start with a simple **SVM** with ***linear*** kernel. We continue to use the **SVM** functionality from the ***sklearn*** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lin = SVC(kernel='linear', C=1E6)\n",
    "svm_lin.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting our model to the training data, the **SVM** functionality from the ***sklearn*** allows us to quickly assess the accuracy of our trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_lin.score(X_test, Y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you noticed, the accuracy is quite high. How does this accuracy make sense in the feature space? Recall how we visualized the decision boundaries for our dummy dataset? Now we would like to visualize the boundary again, but this time, for multi-classes, which is different from the simple case of 2-class classification in the dummy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision(model, ax=None):\n",
    "    '''Plot the decision function for a 2D SVC'''\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 50)\n",
    "    y = np.linspace(ylim[0], ylim[1], 50)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.predict(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contourf(X, Y, P, cmap=plt.cm.coolwarm, alpha=0.2)\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap='Paired', label='training data')\n",
    "plot_svc_decision(svm_lin)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>***Question 0.2: Is the separation is clearly visualized? How would the decision boundaries be like in 3-dimensional feature space? Would they still be lines?***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model evaluation.\n",
    "\n",
    "As you might be wondering when you were playing with the dummy dataset that how the decision boundaries are updated, you can of course apply the same concept to visualize how the decision boundaries evolve with different numbers of iterations. Please note that we are not visualizing how the decision boundaries are updated incrementally, but just how the boundaries look like with different number of iterations!\n",
    "\n",
    "We can also explore further how the **training** will be improving the performance of the model by plotting the iterations of **training** incrementally. A **learning curve** will be a type of such plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply repeat the training procedure with different number of iterations.\n",
    "\n",
    "fig = plt.figure(figsize=(24,8))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    \n",
    "    svm_lin = SVC(kernel='linear', C=1E6, max_iter=i+1)\n",
    "    svm_lin.fit(X_train, Y_train)\n",
    "\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap='Paired', label='training data')\n",
    "    plot_svc_decision(svm_lin)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>***Question 0.3: After few experiments with the number of iteration, how many iterations that you think would be sufficient for this relatively simple dataset?***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further inspect how the accuracy improves with increasing number of iterations, so that we will have an idea of how the **SVM** can be properly or suffciently trained with different number of iterations. Please recall how we used the **confusion matrix** to make simple comparison between predicted and observed data, and how different types of accuracy metrics are derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can first put the predictions on the test dataset in a confusion matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Y_pred = pd.Series(list(svm_lin.predict(X_test)), name='SVM_rbf prediction')  # Store the predicted value in Y_pred\n",
    "Y_actu = pd.Series(list(Y_test), name='Manual delineation')\n",
    "\n",
    "# As before in the session for K-means, we map the LULC codes to the actual name of LULC types\n",
    "\n",
    "# First we need a mapping from the LULC codes to the actual LULC type name.\n",
    "code_lulc = { 52: 'Agriculture',\n",
    "              11: 'Clear water',\n",
    "              61: 'Deciduous forest',\n",
    "              31: 'Residential',\n",
    "              21: 'Sand'}\n",
    "\n",
    "# Now replace the non-intuitive numbers with actual LULC type names and store them into new variables\n",
    "Y_actu2 = Y_actu.replace(code_lulc)\n",
    "Y_pred2 = Y_pred.replace(code_lulc)\n",
    "\n",
    "# Show the LULC coded confusion matrix\n",
    "df_confusion2 = pd.crosstab(Y_actu2, Y_pred2)\n",
    "df_confusion2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course inspect more detailed accuracy by using metrics that we have used in the previous sessions. How does the accuracy look like compared to the accuracy we've already obtained above from the ***svm_lin.score***?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out more detailed accuracy assessment report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report=classification_report(Y_actu2, Y_pred2,output_dict=True)\n",
    "\n",
    "rp = pd.DataFrame(report).transpose()\n",
    "rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>***Exercise 0.1: Please try attach precison and recall as extra row and column to the confusion matrix above to create a more coherent summary of the accuracy assessment.***</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the accuracy changes with different number of iterations.\n",
    "\n",
    "fig = plt.figure(figsize=(24,8))\n",
    "for i in range(3):\n",
    "    \n",
    "    # This time we plot the decision boundaries again the test dataset\n",
    "    plt.subplot(1,3,i+1)\n",
    "    \n",
    "    svm_lin = SVC(kernel='linear', C=1E6, max_iter=i+1)\n",
    "    svm_lin.fit(X_train, Y_train)\n",
    "    \n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap='Paired', label='test data')\n",
    "    plot_svc_decision(svm_lin)\n",
    "    plt.legend()\n",
    "    \n",
    "    Y_pred = pd.Series(list(svm_lin.predict(X_test)), name='SVM_lin prediction')  # Store the predicted value in Y_pred\n",
    "    Y_actu = pd.Series(list(Y_test), name='Manual delineation')\n",
    "    \n",
    "    # Now replace the non-intuitive numbers with actual LULC type names and store them into new variables\n",
    "    Y_actu2 = Y_actu.replace(code_lulc)\n",
    "    Y_pred2 = Y_pred.replace(code_lulc)\n",
    "    \n",
    "    print(classification_report(Y_actu2, Y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>***Question 0.4: How the accuracy assessment captures the visualized classification in the feature space? What are the LULC types are sufficiently or poorly classified for different numbers of iteration, and why?***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, even briefly after 3 iterations the model started to perform properly, the model still performed poorly in the 2 iteration as the ***Agriculture*** and ***Deciduous forest*** are not differentiated properly. In both the accuracy table in terms of the **precision**, **recall**, and the **F1 score**, and the plots, the two classes are poorly predicted on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from looking at the accuracy table and visualizing the decision boundaries, we can also leverage the information from the **learning curve**! This is again provided by the ***sklearn***. The **learning curve** shows how the model is trained incrementally by further split the training data for model **training** and **validation**, hence, essentially, the data has been split into 3 parts! What we have noticed is that the original data has been split explicitly into **training** and **test** sets, but during training, the **training** set would be further and implicitly split for **training** and **validation**, where the model could actually be trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import warnings  # Hide warnings for that the data is very easy to classify and may lead to early termination of iteration\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SVM_lin as before\n",
    "svm_lin = SVC(kernel='linear', C=1E6, max_iter=i+1)\n",
    "\n",
    "# This time we play with an even smaller training dataset reasonable performance has been observed above\n",
    "X_train2, Y_train2, X_test2, Y_test2 = trainTestSplit(data[:,:-1], data[:,-1], 0.8)\n",
    "\n",
    "# Record the training information\n",
    "train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(svm_lin, X_train2, Y_train2, cv=30, return_times=True)\n",
    "\n",
    "# Acquire the recorded details\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Model accuracy')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>***Question 0.5: As you can see, the training and validation accuracy corresponds to each other quite well for this dataset. This is because the data points in the feature space are nicely separated. But you would not expect this to be quite often. Normally, the training accuracy is higher than the validation accuracy. Could you think about why would the training accuracy be higher?***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about **SVM** with non-linear kernel? We would expect the flexiblity of the non-linearity can make more *clean cut* between the data points! Try to inspect how the accuracy will change from using linear kernel to non-linear one? Similarly, we would like to see how the prediction accuracy evolves. Please do try to apply the same procedure of checking accuracy metrics and **learning curve**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = SVC(kernel='rbf', C=1E6)\n",
    "svm_rbf.fit(X_train, Y_train)\n",
    "\n",
    "print(svm_rbf.score(X_test, Y_test)*100)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap='Paired', label='training data')\n",
    "plot_svc_decision(svm_rbf)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>***Exercise 0.2: As you did with K-means, please do try different band combinations to assess the performance of the SVM with different kernels.***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you satisfied with the results on our small **test** dataset? Let's apply the trained model, either the one with **linear** or the one with **non-linear** kernel, to the entire study area covering the large proportion of the Netherlands. # Do you still have the stacked images? We can direclty apply our model to the stack. Otherwise refer back to the part as we were loading and stacking the input bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the image stack\n",
    "# In this case, the more powerful non-linear kernel model is used\n",
    "\n",
    "Y_pred_all = svm_rbf.predict(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then plot the predicted image\n",
    "\n",
    "# Assign color codes to LULC types \n",
    "\n",
    "symbology = {'Agriculture': cols[5],\n",
    "             'Clear water': cols[20],\n",
    "             'Deciduous forest': cols[3],\n",
    "             'Residential': cols[17],\n",
    "             'Sand': cols[11]}\n",
    "\n",
    "# Visualize\n",
    "fig1,(ax1, ax2) = plt.subplots(1,2, figsize=(20,10))\n",
    "show(b5_2020, ax=ax1, cmap='gray', alpha=0.25)\n",
    "aoi.plot(ax=ax1, column='land_cover', legend=True, color=aoi['land_cover'].map(symbology))\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=5, color=color) for color in symbology.values()]\n",
    "leg_points = ax1.legend(custom_points, symbology.keys(), loc='upper right', frameon=False)\n",
    "ax1.add_artist(leg_points)\n",
    "\n",
    "symbology2 = {31: cols[17],\n",
    "              52: cols[5],\n",
    "              11: cols[20],\n",
    "              21: cols[11],\n",
    "              61: cols[3],}\n",
    "\n",
    "# Visualize\n",
    "# Because the predicted labels are still in one column, you need to reshape it back to original image shape\n",
    "row, col = img.shape  # Get the original dimensions of the image\n",
    "imin = min(symbology2)  # Colormap range\n",
    "imax = max(symbology2)\n",
    "\n",
    "print('Printing large image takes time...')\n",
    "ax2.imshow(Y_pred_all.reshape(row, col), cmap=cm, interpolation='none', vmin=imin, vmax=imax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further save the predicted raster map into a georeferenced TIFF, so that we can inspect it along with other data or maps. Please feel free to drag the save TIFF file into other free and open-source software (FOSS) for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we use our image data for georeferencing information\n",
    "rst = rasterio.open('uu_ml/data/b5_2015.TIF')  # Base image to rasterize the *.shp\n",
    "meta = rst.meta.copy()  # Copy metadata from the base image\n",
    "meta.update(compress='lzw')\n",
    "\n",
    "# Burn the AOIs *.shp file into raster and save it\n",
    "out_rst = 'uu_ml/data/tree_prediction.tif'\n",
    "out_file = rasterio.open(\n",
    "    out_rst,\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=row,\n",
    "    width=col,\n",
    "    count=1,\n",
    "    dtype=Y_pred_all.dtype,\n",
    "    crs=rst.crs,\n",
    "    transform=rst.transform)\n",
    "\n",
    "out_file.write(Y_pred_all.reshape(row, col),1)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>***Exercise 0.3: You are again encouraged to compile your own program in a compact and succint manner by putting the pieces you walked through together. You will then inspect the performance with different inputs and model setups. This time, with the SVM as a supervised learning algorithm, you need to try different training data, and see how the results will be influenced by it. Specificically, you need to apply the SVM to all bands and with a more inclusive set of LULC classes, for instance, your own digitized reference data. Rember to constantly take advantage of the accuracy table with different metrics, and the learning curve to inspect the trainig process. And feel free to compare results across models, such as the those from the unsupervised learning algorithms.***</font>\n",
    "\n",
    "*Start to load image bands you would like to use:*\n",
    "\n",
    "```\n",
    "file_list = ['uu_ml/data/b5_2015.TIF', 'uu_ml/data/b6_2015.TIF', ..., ...,]\n",
    "```\n",
    "\n",
    "*Also load different versions of labelled AOIs, and rasterize it:*\n",
    "\n",
    "```\n",
    "aoi = gpd.read_file('...')\n",
    "\n",
    "rst = rasterio.open('...')  # Base image to rasterize the *.shp\n",
    "meta = rst.meta.copy()  # Copy metadata from the base image\n",
    "meta.update(compress='lzw')\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "*Train your model and inspect the training processes*\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import learning_curve\n",
    "    ...\n",
    "    ...\n",
    "```\n",
    "\n",
    "*Evaluate the accuracy with different metrics*\n",
    "\n",
    "```\n",
    "...\n",
    "...\n",
    "```\n",
    "\n",
    "*Generalize your prediction from the trained model to larger areas*\n",
    "\n",
    "```\n",
    "...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
